{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FUWrSgzc9DE",
        "outputId": "feb2c2c0-07c9-411d-d2cc-743b846394aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive using this statement\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZFta5K6ix3t",
        "outputId": "759b77d4-5adf-410c-f5a3-16e51d66f63e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create directory for cloning yolov7 \n",
        "import os\n",
        "\n",
        "if not os.path.isdir(\"MidPrepCloudPhysician\"):\n",
        "  os.makedirs(\"MidPrepCloudPhysician\")"
      ],
      "metadata": {
        "id": "ZiEeM7FajtEc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# folder where yolov7 will clone\n",
        "%cd /content/gdrive/MyDrive/MidPrepCloudPhysician"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu2ggi0NxJMd",
        "outputId": "cefa24a6-e645-4d7e-f3b1-a6d14ce9119f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/MidPrepCloudPhysician\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clonning yolov7\n",
        "!git clone https://github.com/WongKinYiu/yolov7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwzw4BC7kXCH",
        "outputId": "16e85803-3c39-4335-e1d0-1cf5c0d891f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov7'...\n",
            "remote: Enumerating objects: 1127, done.\u001b[K\n",
            "remote: Total 1127 (delta 0), reused 0 (delta 0), pack-reused 1127\u001b[K\n",
            "Receiving objects: 100% (1127/1127), 69.93 MiB | 14.60 MiB/s, done.\n",
            "Resolving deltas: 100% (522/522), done.\n",
            "Updating files: 100% (104/104), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# yolov7 folder\n",
        "%cd /content/gdrive/MyDrive/MidPrepCloudPhysician/yolov7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMPp331YxO2W",
        "outputId": "efb25238-96d7-4545-9df8-baf9acdbd78c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/MidPrepCloudPhysician/yolov7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "import gdown\n",
        "# Download features model weights https://drive.google.com/file/d/16GUeuwY6ezlMxXAlzwBOnVIBHLd2Vivg/view?usp=sharing\n",
        "gdown.download(\"https://drive.google.com/uc?id=16GUeuwY6ezlMxXAlzwBOnVIBHLd2Vivg&confirm=t\", \"Feature_weights.pt\", quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "N6pCnpM6koN3",
        "outputId": "82ce1b76-7272-48de-f9c9-a08136bae443"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16GUeuwY6ezlMxXAlzwBOnVIBHLd2Vivg&confirm=t\n",
            "To: /content/gdrive/MyDrive/MidPrepCloudPhysician/yolov7/Feature_weights.pt\n",
            "100%|██████████| 74.8M/74.8M [00:01<00:00, 41.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Feature_weights.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download monitor model weights https://drive.google.com/file/d/1qcnxbJ9h9EfKf9K6BDUZBDGCnP2VGxbA/view?usp=sharing\n",
        "gdown.download(\"https://drive.google.com/uc?id=1qcnxbJ9h9EfKf9K6BDUZBDGCnP2VGxbA&confirm=t\", \"monitor_weights.pt\", quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "_PmeSqTiwBWD",
        "outputId": "904d644b-15b7-4d64-b58c-88388ba8f579"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qcnxbJ9h9EfKf9K6BDUZBDGCnP2VGxbA&confirm=t\n",
            "To: /content/gdrive/MyDrive/MidPrepCloudPhysician/yolov7/monitor_weights.pt\n",
            "100%|██████████| 74.8M/74.8M [00:01<00:00, 52.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'monitor_weights.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh36LFZDdKc3",
        "outputId": "f76843c8-a83a-4879-aea1-03351fafcc25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.6.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from easyocr) (1.7.3)\n",
            "Collecting opencv-python-headless<=4.5.4.60\n",
            "  Downloading opencv_python_headless-4.5.4.60-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from easyocr) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from easyocr) (7.1.2)\n",
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.8/dist-packages (from easyocr) (2.0.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from easyocr) (6.0)\n",
            "Collecting pyclipper\n",
            "  Downloading pyclipper-1.3.0.post4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (619 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.2/619.2 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.8/dist-packages (from easyocr) (0.14.1+cu116)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from easyocr) (1.13.1+cu116)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from easyocr) (0.18.3)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.5->easyocr) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.5->easyocr) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from python-bidi->easyocr) (1.15.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->easyocr) (3.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->easyocr) (1.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->easyocr) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->easyocr) (2023.1.23.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->easyocr) (2.9.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (1.4.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5->easyocr) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5->easyocr) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5->easyocr) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5->easyocr) (2022.12.7)\n",
            "Installing collected packages: pyclipper, ninja, python-bidi, opencv-python-headless, easyocr\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.7.0.68\n",
            "    Uninstalling opencv-python-headless-4.7.0.68:\n",
            "      Successfully uninstalled opencv-python-headless-4.7.0.68\n",
            "Successfully installed easyocr-1.6.2 ninja-1.11.1 opencv-python-headless-4.5.4.60 pyclipper-1.3.0.post4 python-bidi-0.4.2\n"
          ]
        }
      ],
      "source": [
        " pip install easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kQMjLWfMdNQY"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.vq import kmeans\n",
        "import easyocr\n",
        "import ast\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/MidPrepCloudPhysician/yolov7/models')  \n",
        "import models\n",
        "import cv2\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "import copy \n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sassKUjcdTs1"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "drt3M3CGdgul"
      },
      "outputs": [],
      "source": [
        "\n",
        "classes_to_filter = [] \n",
        "\n",
        "# Monitor extraction model parameters\n",
        "opt_monitor  = {\n",
        "    \n",
        "    \"weights\": \"/content/gdrive/MyDrive/MidPrepCloudPhysician/yolov7/monitor_weights.pt\", \n",
        "    \"yaml\"   : \"data/custom_data.yaml\",\n",
        "    \"img-size\": 640, \n",
        "    \"conf-thres\": 0.25, \n",
        "    \"iou-thres\" : 0.45, \n",
        "    \"device\" : 'cpu',  \n",
        "    \"classes\" : classes_to_filter  \n",
        "\n",
        "}\n",
        "\n",
        "# Feature extraction model parameters \n",
        "opt_features  = {\n",
        "    \n",
        "    \"weights\": \"/content/gdrive/MyDrive/MidPrepCloudPhysician/yolov7/Feature_weights.pt\", \n",
        "    \"yaml\"   : \"data/custom_data.yaml\",\n",
        "    \"img-size\": 640, \n",
        "    \"conf-thres\": 0.25, \n",
        "    \"iou-thres\" : 0.45, \n",
        "    \"device\" : 'cpu',  \n",
        "    \"classes\" : classes_to_filter  \n",
        "\n",
        "}\n",
        "\n",
        "# Class labels for feature extraction model\n",
        "feature_classes = {\n",
        "    0:\"HR\",\n",
        "    1:\"SBP\",\n",
        "    2:\"DBP\",\n",
        "    3:\"MAP\",\n",
        "    4:\"SPO2\",\n",
        "    5:\"RR\",\n",
        "    6:\"HR_W\",\n",
        "    7:\"RR_W\", \n",
        "    8:\"SPO2_W\"\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BGds2DiEdivo"
      },
      "outputs": [],
      "source": [
        "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
        "    '''\n",
        "    Function responsible for image preprocessing.\n",
        "    Args: \n",
        "      img: image file  \n",
        "    Returns:\n",
        "      img: Processed image\n",
        "      ratio: Image Scale ratio \n",
        "      (dw,dh)\n",
        "    '''\n",
        "\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = img.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return img, ratio, (dw, dh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OEJyi7YHdnUt"
      },
      "outputs": [],
      "source": [
        "# Function for loading models\n",
        "def Load_model(opt):\n",
        "  '''\n",
        "    Function responsible for loading models.\n",
        "    Args: \n",
        "      opt: Model parameters, dict\n",
        "    Returns:\n",
        "      model: Loaded model\n",
        "  '''\n",
        "\n",
        "  with torch.no_grad():\n",
        "    weights, imgsz = opt['weights'], opt['img-size']\n",
        "    set_logging()\n",
        "    device = select_device(opt['device'])\n",
        "    half = device.type != 'cpu'\n",
        "    model = attempt_load(weights, map_location=device)  \n",
        "  \n",
        "    if half:\n",
        "      model.half()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7z5kSyQAKlP",
        "outputId": "73c119b4-60a7-4592-f946-91e151b82b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n"
          ]
        }
      ],
      "source": [
        "# loading the monitor screen extraction model\n",
        "monitor_model = Load_model(opt_monitor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U96mDJ7veSgX",
        "outputId": "3d8b057a-0b74-49d4-fb78-a3a8110a2f22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n"
          ]
        }
      ],
      "source": [
        "# loading the feature extraction(from monitor screen) model \n",
        "feature_model = Load_model(opt_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DMos-f6HeXyE"
      },
      "outputs": [],
      "source": [
        "def get_predictions(image, model, opt):\n",
        "  '''\n",
        "  Function responsible for making predictions from model.\n",
        "  Args:  \n",
        "    image: image file\n",
        "    model: Loaded model\n",
        "    opt: Model Parameters, dict\n",
        "  Returns:\n",
        "    img0: Labelled image with predicted bounding boxes, image file\n",
        "    pred: Model predictions, Tensor\n",
        "  '''\n",
        "  img0 = copy.copy(image)\n",
        "  imgsz = opt['img-size']\n",
        "  stride = int(model.stride.max())\n",
        "  device = select_device(opt['device'])\n",
        "  half = device.type != 'cpu'\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "  img = letterbox(img0, imgsz, stride=stride)[0]\n",
        "  img = img[:, :, ::-1].transpose(2, 0, 1)  \n",
        "  img = np.ascontiguousarray(img)\n",
        "  img = torch.from_numpy(img).to(device)\n",
        "  img = img.half() if half else img.float()  \n",
        "  img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "  if img.ndimension() == 3:\n",
        "    img = img.unsqueeze(0)\n",
        "  t1 = time_synchronized()\n",
        "  pred = model(img, augment= False)[0]\n",
        "\n",
        "  classes = None\n",
        "  if opt['classes']:\n",
        "    classes = []\n",
        "    for class_name in opt['classes']:\n",
        "\n",
        "      classes.append(names.index(class_name))\n",
        "\n",
        "  if classes:\n",
        "    \n",
        "    classes = [i for i in range(len(names)) if i not in classes]\n",
        "  \n",
        "  pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= None, agnostic= False)\n",
        "  t2 = time_synchronized()\n",
        "  for i, det in enumerate(pred):\n",
        "    s = ''\n",
        "    s += '%gx%g ' % img.shape[2:]  \n",
        "    gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
        "    if len(det):\n",
        "      det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "\n",
        "      for c in det[:, -1].unique():\n",
        "        n = (det[:, -1] == c).sum()  \n",
        "        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  \n",
        "    \n",
        "      for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "        label = f'{names[int(cls)]} {conf:.2f}'\n",
        "        plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=1)\n",
        "\n",
        "  return img0, pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rTHfS691g2jv"
      },
      "outputs": [],
      "source": [
        "def get_cropped_feature_image(image, model = monitor_model, model1 = feature_model, opt = opt_monitor, opt1 = opt_features):\n",
        "  '''\n",
        "  Function responsible for getting monitor screen, feature predicted image and its predictions.\n",
        "  Args:  \n",
        "    image: image file\n",
        "    model: Monitor extraction model\n",
        "    model1 = feature extraction model  \n",
        "    opt: Monitor extraction model Parameters, dict\n",
        "    opt1: feature extraction model Parameters, dict\n",
        "  Returns:\n",
        "    cropped: Cropped image(monitor image), image file\n",
        "    img1: feature predicted monitor image\n",
        "    pred1: feature predictions, Tensor\n",
        "  ''' \n",
        "  img0, pred = get_predictions(image, model, opt)\n",
        "  pred_np = pred[0][:, :6].cpu().detach().numpy()\n",
        "  x, y, w, h = int(pred_np[0][0]), int(pred_np[0][1]), int(pred_np[0][2]), int(pred_np[0][3])\n",
        "  cropped = img0[y:h,x:w]\n",
        "  img1, pred1 = get_predictions(cropped, model1, opt1)\n",
        "  \n",
        "  return cropped, img1, pred1\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "knl3O_x9DI0U"
      },
      "outputs": [],
      "source": [
        "def features_dataframe(pred):\n",
        "  '''\n",
        "  Function responsible for converting feature prediction tensor to dataframe.\n",
        "  Args:  \n",
        "    pred: Feature prediction tensor \n",
        "  Returns:\n",
        "    df: Pandas dataframe containing bounding box coordinates of predicted features in monitor screen\n",
        "    confidence_scrore: Stores highest confidence score of each predicted class, dict \n",
        "  '''\n",
        "  pred_np = pred[0][:, :6].cpu().detach().numpy() # Numpy array of all predictions\n",
        "  columns = ['image_name', 'HR', 'SBP', 'DBP', 'MAP', 'SPO2', 'RR', 'HR_W', 'RR_W', 'SPO2_W']\n",
        "  df = pd.DataFrame(columns=columns)\n",
        "  df = df.append({'image_name': 'test_image'}, ignore_index=True) \n",
        "\n",
        "  no_rows = pred_np.shape[0]\n",
        "\n",
        "  labels=[]\n",
        "  coords=[]\n",
        "  confidence_score = {}\n",
        "  for i in range(0,no_rows):\n",
        "    pred_arr = pred_np[i]\n",
        "    label = pred_arr[5]\n",
        "    conf = pred_arr[4]\n",
        "    if conf > confidence_score.get(label,0):\n",
        "      confidence_score[label] = conf\n",
        "      coordinates = pred_arr[0:4]\n",
        "      coord = []\n",
        "      for j in coordinates:\n",
        "        coord.append(j)\n",
        "      df.loc[0,feature_classes[label]] = f'{coord}'\n",
        "  \n",
        "  return df, confidence_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading easyocr model\n",
        "reader = easyocr.Reader(['en'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ4mzllCb9kV",
        "outputId": "7e03d628-d8e1-4dc7-ec0c-a2b77f65e08d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(img, df):\n",
        "    '''\n",
        "    Function responsible for detecting numbers from the predicted feature image.\n",
        "    Args:  \n",
        "      img: Monitor screen image\n",
        "      df: Pandas dataframe containing bounding box coordinates of predicted features in monitor screen\n",
        "    Returns:\n",
        "      output: Output dictionary containing feature numbers for each class, eg: {'HR': '102', 'SPO2': '96', 'RR': '18'}\n",
        "      \n",
        "    '''\n",
        "  \n",
        "    output = {}\n",
        "    lst = ['HR', 'SBP', 'DBP', 'MAP', 'SPO2', 'RR']\n",
        "    \n",
        "    for j in range(6):\n",
        "      if pd.isna(df.iat[0, j+1]) == False:\n",
        "          x1,y1,x2,y2 = ast.literal_eval(df.iat[0, j+1])\n",
        "          cropped_img = img[int(y1):int(y2), int(x1):int(x2)]\n",
        "          result = reader.readtext(cropped_img)\n",
        "          if len(result) != 0:\n",
        "            cleaned_string = \"\"\n",
        "            for char in result[0][1]:\n",
        "              try:\n",
        "                int(char)\n",
        "                cleaned_string += char\n",
        "              except ValueError:\n",
        "                pass\n",
        "            print(f\"{lst[j]} identified\")\n",
        "            if (j == 1):\n",
        "              if len(cleaned_string)== 4:\n",
        "                output[lst[j]] = cleaned_string[:3]\n",
        "              else:\n",
        "                output[lst[j]] = cleaned_string\n",
        "            else:\n",
        "                output[lst[j]] = cleaned_string\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "ojwe9BwpcA80"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0LCibUePdW1u"
      },
      "outputs": [],
      "source": [
        "# Main inference Function. Only provide input image path to this function \n",
        "def inference(image_path, mon_model = monitor_model, fea_model = feature_model,opt = opt_monitor, opt1 = opt_features): \n",
        "  '''\n",
        "  Function responsible for inference.\n",
        "  Args:  \n",
        "    image_path: Path of the test image\n",
        "    \"Rest all inputs are default parameters. Don't change them\"\n",
        "  Returns:\n",
        "    output_df: Output dictionary containing feature numbers for each class, eg: {'HR': '102', 'SPO2': '96', 'RR': '18'}\n",
        "  '''\n",
        "  image = cv2.imread(image_path)\n",
        "  monitor_image, img1, pred1 = get_cropped_feature_image(image, model = mon_model, model1 = fea_model, opt = opt_monitor, opt1 = opt_features)\n",
        "  df, confi = features_dataframe(pred1)\n",
        "  output_df = pipeline(monitor_image, df)\n",
        "  return output_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running inference on test image \n",
        "output = inference('Test image path') # Just enter the path of the test image \n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIuVCP9ff3Rn",
        "outputId": "1affffbf-f6b6-4472-ab2e-b91ba59920bc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HR identified\n",
            "SBP identified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'HR': '111', 'SBP': '100'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graph digitization \n"
      ],
      "metadata": {
        "id": "jwkarUOzfRpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading and processing data for graph digitization\n",
        "image_path = \"Image Path\" # Enter the same test image path as mentioned above in inference function\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "img0, pred = get_predictions(image, monitor_model, opt_monitor)\n",
        "pred_np = pred[0][:, :6].cpu().detach().numpy()\n",
        "x, y, w, h = int(pred_np[0][0]), int(pred_np[0][1]), int(pred_np[0][2]), int(pred_np[0][3])\n",
        "cropped_image = img0[y:h,x:w]\n",
        "#cv2_imshow(cropped_image)\n",
        "img1, pred1 = get_predictions(cropped_image, feature_model, opt_features)\n",
        "df, confi = features_dataframe(pred1)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "qmC09LsOgOFH",
        "outputId": "f1ed78af-14a2-411d-9cd8-1f13521ba600"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image_name                          HR                          SBP  DBP  \\\n",
              "0  test_image  [334.0, 50.0, 382.0, 85.0]  [327.0, 88.0, 380.0, 122.0]  NaN   \n",
              "\n",
              "                           MAP SPO2   RR                       HR_W RR_W  \\\n",
              "0  [76.0, 268.0, 138.0, 289.0]  NaN  NaN  [13.0, 44.0, 367.0, 74.0]  NaN   \n",
              "\n",
              "                       SPO2_W  \n",
              "0  [0.0, 222.0, 336.0, 268.0]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2714183d-ca16-4fca-ab99-f8894adb5a0d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>HR</th>\n",
              "      <th>SBP</th>\n",
              "      <th>DBP</th>\n",
              "      <th>MAP</th>\n",
              "      <th>SPO2</th>\n",
              "      <th>RR</th>\n",
              "      <th>HR_W</th>\n",
              "      <th>RR_W</th>\n",
              "      <th>SPO2_W</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test_image</td>\n",
              "      <td>[334.0, 50.0, 382.0, 85.0]</td>\n",
              "      <td>[327.0, 88.0, 380.0, 122.0]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[76.0, 268.0, 138.0, 289.0]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[13.0, 44.0, 367.0, 74.0]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[0.0, 222.0, 336.0, 268.0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2714183d-ca16-4fca-ab99-f8894adb5a0d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2714183d-ca16-4fca-ab99-f8894adb5a0d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2714183d-ca16-4fca-ab99-f8894adb5a0d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_digitise(img, df):\n",
        "    '''\n",
        "    Function responsible for graph digitization.\n",
        "    Args:  \n",
        "      img: Monitor screen image\n",
        "      df: Pandas dataframe containing bounding box coordinates of predicted features in monitor screen\n",
        "    Returns:\n",
        "      fig: Digitized graph\n",
        "    '''\n",
        "    for j in range(6,7):\n",
        "      if pd.isna(df.iat[0, j+1]) == False:\n",
        "          x1,y1,x2,y2 = ast.literal_eval(df.iat[0, j+1])\n",
        "          cropped_img = img[int(y1):int(y2), int(x1):int(x2)]\n",
        "          pixels = np.array(cropped_img)\n",
        "          \n",
        "          ## Thresholding\n",
        "          pixels_2d = pixels.reshape(-1, pixels.shape[-1]).astype(np.float32)\n",
        "          black_threshold = 10\n",
        "          black_pixels = pixels_2d[np.all(pixels_2d <= black_threshold, axis=1)]\n",
        "          non_black_pixels = pixels_2d[np.any(pixels_2d > black_threshold, axis=1)]\n",
        "          num_colors = 1\n",
        "          centroids, _ = kmeans(non_black_pixels, num_colors)\n",
        "          \n",
        "          ## Creating mask and filling values \n",
        "          value = centroids[0]\n",
        "          range_ = 55\n",
        "          mask = np.logical_or(pixels[..., 0] < value[0]-range_, pixels[..., 0] > value[0]+range_)\n",
        "          mask = np.logical_or(mask, np.logical_or(pixels[..., 1] < value[1]-range_, pixels[..., 1] > value[1]+range_))\n",
        "          mask = np.logical_or(mask, np.logical_or(pixels[..., 2] < value[2]-range_, pixels[..., 2] > value[2]+range_))\n",
        "          extracted_portion = np.where(mask[..., np.newaxis], [0, 255, 0], [0, 0, 0])\n",
        "          \n",
        "          ## Getting co-ordinates\n",
        "          img = cv2.convertScaleAbs(extracted_portion, alpha=(255.0))\n",
        "          _, threshold = cv2.threshold(img, 100, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "          kernel = np.ones((1, 1), np.uint8)\n",
        "          threshold = cv2.morphologyEx(threshold, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "          coords = np.column_stack(np.where(extracted_portion > 0))\n",
        "          x = coords[:, 1]\n",
        "          y = coords[:, 0]\n",
        "          df = pd.DataFrame({'x': x, 'y': y})\n",
        "          df = df.drop_duplicates()\n",
        "\n",
        "          ## Using DBscan clustering algo\n",
        "          db = DBSCAN(eps=5, min_samples=5).fit(np.column_stack([x, y]))\n",
        "          core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "          core_samples_mask[db.core_sample_indices_] = True\n",
        "          unique_labels = set(db.labels_)\n",
        "          colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
        "          plt.figure(figsize=(10,1), dpi=100)\n",
        "          fig, ax = plt.subplots(figsize=(10,1))\n",
        "          for k, col in zip(unique_labels, colors):\n",
        "            class_member_mask = (db.labels_ == k)\n",
        "            xy = np.column_stack([x[class_member_mask & core_samples_mask], y[class_member_mask & core_samples_mask]])\n",
        "            ax.plot(xy[:, 0], extracted_portion.shape[0]-xy[:, 1], '.', markerfacecolor=tuple(col), markersize=2)\n",
        "            plt.show()\n",
        "          return fig\n",
        "      else:\n",
        "        return ('HR not found')"
      ],
      "metadata": {
        "id": "5yswD8h3cJPR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the digitized graph\n",
        "fig = graph_digitise(cropped_image, df)\n",
        "fig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "7-qUvoV5yyqb",
        "outputId": "c4c54d27-1f18-4917-b7f3-ffe91ddfcfae"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 720x72 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTxxpA9J_Anj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}